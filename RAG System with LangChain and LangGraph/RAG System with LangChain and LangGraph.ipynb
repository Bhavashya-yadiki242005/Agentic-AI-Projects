{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22629a3-d8ae-463a-8598-35a766ac898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Install Dependencies\n",
    "# Install required libraries:\n",
    "# - langchain: document loading, splitting, embeddings, LLM interface\n",
    "# - langgraph: workflow orchestration\n",
    "# - langchain-openai: OpenAI integrations\n",
    "# - networkx + matplotlib: workflow visualization\n",
    "\n",
    "!pip install langchain langgraph langchain-openai langchain-text-splitters langchain-community networkx matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f56da-58d5-4def-8ae3-1aede3a3dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup OpenAI API Key\n",
    "# Store OpenAI API key as environment variable\n",
    "# LangChain automatically reads this variable when calling models\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec08315-67fa-40f7-bee6-e89ddcc2a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Define Application State\n",
    "# State defines data flowing through the LangGraph pipeline\n",
    "# question: user input\n",
    "# context: retrieved documents\n",
    "# answer: generated LLM response\n",
    "\n",
    "from typing_extensions import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fb555-773c-48ae-816b-10788f8c6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Load Knowledge Base\n",
    "# Load external knowledge from JSON file\n",
    "# Each entry becomes a LangChain Document object\n",
    "\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "with open('knowledge_base.json', 'r') as f:\n",
    "    knowledge_items = json.load(f)\n",
    "\n",
    "local_docs = [Document(page_content=item['text']) for item in knowledge_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ea343-1f8d-4ea9-bfe9-f28dac8c014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Create Embeddings + Vector Store\n",
    "# Convert documents into vector embeddings\n",
    "# Store them in an in-memory vector database for similarity search\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# Store all document chunks as vectors\n",
    "vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9df3b2-660d-41fc-ae74-05d93bb3193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Prompt Template + LLM Initialization\n",
    "# Custom prompt forces LLM to use retrieved context\n",
    "# Prevents hallucination\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "CUSTOM_PROMPT = \"\"\"\n",
    "You are an advanced assistant. Use the context to answer. If insufficient info, say so clearly.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Initialize GPT model with controlled creativity\n",
    "llm = init_chat_model(\"openai:gpt-4.1\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa375f-54da-4b7f-a677-57f40a1acdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Workflow Functions (LangGraph Nodes)\n",
    "# Retrieves top 5 most relevant document chunks for user query\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"], k=5)\n",
    "    return {\"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849590a-aa56-430c-8758-fc6e5d8bf559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Answer\n",
    "# Builds final prompt using retrieved context\n",
    "# Sends to LLM and returns response\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    prompt_filled = CUSTOM_PROMPT.format(\n",
    "        question=state[\"question\"], context=docs_content)\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": prompt_filled}])\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f009f-8756-43c6-9eaa-83066724248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify Question (Optional Extension Point)\n",
    "# Placeholder classifier for routing logic\n",
    "# Currently passes question unchanged\n",
    "\n",
    "def classify(state: State):\n",
    "    is_advanced = \"advanced\" in state[\"question\"].lower()\n",
    "    return {\"question\": state[\"question\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a948dd-2a09-4459-8127-269aa2e73208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refine Answer\n",
    "# Adds refinement note to improve clarity\n",
    "\n",
    "def refine(state: State):\n",
    "    refined_answer = state[\"answer\"] + \"\\n\\n[Refined for clarity and completeness]\"\n",
    "    return {\"answer\": refined_answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5441da-2a03-4d94-aeed-c524dd318ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Build LangGraph Workflow\n",
    "# Create directed workflow graph\n",
    "# Order: classify → retrieve → generate → refine\n",
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence(\n",
    "    [classify, retrieve, generate, refine]\n",
    ")\n",
    "\n",
    "# Connect start node\n",
    "graph_builder.add_edge(START, \"classify\")\n",
    "\n",
    "# Compile graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d7d80-3fb2-45e8-8467-c4a63b2bfe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Visualize Workflow\n",
    "# Draw workflow graph using NetworkX + Matplotlib\n",
    "# Helps understand RAG pipeline visually\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_langgraph_clean(graph_builder):\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for node_name in graph_builder.nodes:\n",
    "        G.add_node(node_name)\n",
    "\n",
    "    for src, tgt in graph_builder.edges:\n",
    "        G.add_edge(src, tgt)\n",
    "\n",
    "    try:\n",
    "        pos = nx.nx_agraph.graphviz_layout(G, prog='dot')\n",
    "    except:\n",
    "        pos = nx.spring_layout(G)\n",
    "\n",
    "    nx.draw(G, pos, with_labels=True)\n",
    "    plt.show()\n",
    "\n",
    "visualize_langgraph_clean(graph_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5d48e-829d-4bb5-8ccd-ef80d746a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Run Interactive RAG System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4977994-0312-48f2-b938-4d8409ebff03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
